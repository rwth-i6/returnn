#!rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:


import os
from subprocess import check_output
import numpy

my_dir = os.path.dirname(os.path.abspath(__file__))

debug_mode = False
if int(os.environ.get("XDEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# task
use_tensorflow = True
task = 'train'
device = 'gpu'


# data
cache_size = "0"
# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train'}"
num_inputs = 46300
num_outputs = {'classes': [34908, 1], 'data': [num_inputs, 1]}
# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train', 'seq_ordering':'sorted'}" --get_num_seqs
num_seqs = {'train': 32799253}
EpochSplit = 100
SeqOrderTrainBins = num_seqs["train"] // 1000
TrainSeqOrder = "laplace:%i" % SeqOrderTrainBins
if debug_mode:
    TrainSeqOrder = "default"


def _old_get_dataset(data):
    epochSplit = {"train": EpochSplit}.get(data, 1)
    return {
        'class': 'TranslationDataset',
        'path': 'base/dataset',
        'use_cache_manager': True,
        'file_postfix': data,
        'partition_epoch': epochSplit,
        "seq_ordering": {"train": TrainSeqOrder, "dev": "sorted"}.get(data, "default"),
        'source_postfix': ' </S>',
        'target_postfix': ' </S>',
        'unknown_label': '<UNK>',
        "estimated_num_seqs": (num_seqs.get(data, None) // epochSplit) if data in num_seqs else None
    }

def get_dataset(data):
    epochSplit = {"train": EpochSplit}.get(data, 1)
    return {
        'class': 'HDFDataset',
        'files': ['base/dataset-hdf/data/%s.hdf' % data],
        'use_cache_manager': True,
        'partition_epoch': epochSplit,
        "seq_ordering": {"train": TrainSeqOrder, "dev": "sorted"}.get(data, "default")
    }

train = get_dataset("train")
dev = get_dataset("dev")
eval_data = get_dataset("eval")

# network
# (also defined by num_inputs & num_outputs)

class TransformerNetwork:

    def __init__(self, encN=6, decN=6, rnn_dec=False, only_one_enc_dec_att='', normalized_loss = False):
        self.encN = encN
        self.decN = decN
        self.FFDim = 2048
        self.EncKeyTotalDim = 512
        self.AttNumHeads = 8
        self.EncKeyPerHeadDim = self.EncKeyTotalDim // self.AttNumHeads
        self.EncValueTotalDim = 512
        self.EncValuePerHeadDim = self.EncValueTotalDim // self.AttNumHeads
        self.embed_weight = self.EncValueTotalDim**0.5
        self.rnn_dec = rnn_dec
        self.only_one_enc_dec_att = only_one_enc_dec_att
        self.normalized_loss = normalized_loss

        self.embed_dropout = 0.0
        self.postprocess_dropout = 0.1
        self.act_dropout = 0.1
        self.attention_dropout = 0.1
        self.label_smoothing = 0.1

        self.ff_init = "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=0.78)"

    def add_trafo_enc_layer(self, d, inp, output):
        d[output + '_self_att_laynorm'] = {"class": "layer_norm", "from": [inp]}
        d[output + '_self_att_att'] = {"class": "self_attention", "num_heads": self.AttNumHeads,
                                    "total_key_dim": self.EncKeyTotalDim,
                                    "n_out": self.EncValueTotalDim, "from": [output + '_self_att_laynorm'],
                                    "attention_left_only": False, "attention_dropout": self.attention_dropout, "forward_weights_init": self.ff_init}
        d[output + '_self_att_lin'] = {"class": "linear", "activation": None, "with_bias": False,
                                       "from": [output + '_self_att_att'], "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_self_att_drop'] = {"class": "dropout", "from": [output + '_self_att_lin'], "dropout": self.postprocess_dropout}
        d[output + '_self_att_out'] = {"class": "combine", "kind": "add", "from": [inp, output + '_self_att_drop'],
                                       "n_out": self.EncValueTotalDim}
        #####
        d[output + '_ff_laynorm'] = {"class": "layer_norm", "from": [output + '_self_att_out']}
        d[output + '_ff_conv1'] = {"class": "linear", "activation": "relu", "with_bias": True,
                                   "from": [output + '_ff_laynorm'],
                                   "n_out": self.FFDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_conv2'] = {"class": "linear", "activation": None, "with_bias": True,
                                   "from": [output + '_ff_conv1'], "dropout": self.act_dropout,
                                   "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_ff_conv2'], "dropout": self.postprocess_dropout}
        d[output + '_ff_out'] = {"class": "combine", "kind": "add",
                                 "from": [output + '_self_att_out', output + '_ff_drop'],
                                 "n_out": self.EncValueTotalDim}
        d[output] = {"class": "copy", "from": [output + '_ff_out']}

    def add_trafo_dec_layer(self, db, d, inp, output, reuse_att=None):
        pre_inp = [inp, 'prev:' + output + '_att_drop'] if self.rnn_dec else [inp]
        d[output + '_self_att_laynorm'] = {"class": "layer_norm", "from": pre_inp}
        d[output + '_self_att_att'] = {"class": "rnn_cell", "unit": "LSTMBlock",
                "from": [output + '_self_att_laynorm'], "n_out": self.EncValueTotalDim} if self.rnn_dec else {"class": "self_attention", "num_heads": self.AttNumHeads,
                                    "total_key_dim": self.EncKeyTotalDim,
                                    "n_out": self.EncValueTotalDim, "from": [output + '_self_att_laynorm'],
                                    "attention_left_only": True, "attention_dropout": self.attention_dropout, "forward_weights_init": self.ff_init}
        d[output + '_self_att_lin'] = {"class": "linear", "activation": None, "with_bias": False,
                                       "from": [output + '_self_att_att'], "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_self_att_drop'] = {"class": "dropout", "from": [output + '_self_att_lin'], "dropout": self.postprocess_dropout}
        d[output + '_self_att_out'] = {"class": "combine", "kind": "add", "from": [inp, output + '_self_att_drop'],
                                       "n_out": self.EncValueTotalDim}
        #####
        if not reuse_att:
            d[output + '_att_laynorm'] = {"class": "layer_norm", "from": [output + '_self_att_out']}
            d[output + '_att_query0'] = {"class": "linear", "activation": None, "with_bias": False,
                                         "from": [output + '_att_laynorm'],
                                         "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
            d[output + '_att_query'] = {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncKeyPerHeadDim),
                                        "from": [output + '_att_query0']}  # (B, H, D/H)
            db[output + '_att_key0'] = {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"],
                                        "n_out": self.EncKeyTotalDim, "forward_weights_init": self.ff_init}  # (B, enc-T, D)
            db[output + '_att_value0'] = {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"],
                                          "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
            db[output + '_att_key'] = {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncKeyPerHeadDim),
                                       "from": [output + '_att_key0']}  # (B, enc-T, H, D/H)
            db[output + '_att_value'] = {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncValuePerHeadDim),
                                         "from": [output + '_att_value0']}  # (B, enc-T, H, D'/H)
            d[output + '_att_energy'] = {"class": "dot", "red1": -1, "red2": -1, "var1": "T", "var2": "T?",
                                         "from": ['base:' + output + '_att_key', output + '_att_query']}  # (B, H, enc-T, 1)
            d[output + '_att_weights'] = {"class": "softmax_over_spatial", "from": [output + '_att_energy'],
                                          "energy_factor": self.EncKeyPerHeadDim ** -0.5}  # (B, enc-T, H, 1)

            d[output + '_att_weights_drop'] = {"class": "dropout", "dropout_noise_shape": {"*": None},
                                               "from": [output + '_att_weights'], "dropout": self.attention_dropout}

            d[output + '_att0'] = {"class": "generic_attention", "weights": output + '_att_weights_drop',
                                   "base": 'base:' + output + '_att_value'}  # (B, H, V)
            d[output + '_att_att'] = {"class": "merge_dims", "axes": "static",
                                   "from": [output + '_att0']}  # (B, H*V) except_batch
            d[output + '_att_lin'] = {"class": "linear", "activation": None, "with_bias": False, "from": [output + '_att_att'],
                                      "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
            d[output + '_att_drop'] = {"class": "dropout", "from": [output + '_att_lin'], "dropout": self.postprocess_dropout}
            d[output + '_att_out'] = {"class": "combine", "kind": "add",
                                      "from": [output + '_self_att_out', output + '_att_drop'],
                                      "n_out": self.EncValueTotalDim}
        elif self.only_one_enc_dec_att == 'add':
            d[output + '_att_out'] = {"class": "combine", "from": [output + '_self_att_out', reuse_att + '_att_drop'],
                    "n_out": self.EncValueTotalDim, "kind": "add"}
        elif self.only_one_enc_dec_att == 'concat':
            d[output + '_att_out'] = {"class": "linear", "from": [output + '_self_att_out', reuse_att + '_att_drop'],
                    "n_out": self.EncValueTotalDim, "activation": None, "with_bias": False, "forward_weights_init": self.ff_init}
        #####
        d[output + '_ff_laynorm'] = {"class": "layer_norm", "from": [output + '_att_out']}
        d[output + '_ff_conv1'] = {"class": "linear", "activation": "relu", "with_bias": True,
                                   "from": [output + '_ff_laynorm'],
                                   "n_out": self.FFDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_conv2'] = {"class": "linear", "activation": None, "with_bias": True,
                                   "from": [output + '_ff_conv1'], "dropout": self.act_dropout,
                                   "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_ff_conv2'], "dropout": self.postprocess_dropout}
        d[output + '_ff_out'] = {"class": "combine", "kind": "add", "from": [output + '_att_out', output + '_ff_drop'],
                                 "n_out": self.EncValueTotalDim}
        d[output] = {"class": "copy", "from": [output + '_ff_out']}

    def build(self):
        network = {
            "source_embed_raw": {"class": "linear", "activation": None, "with_bias": False, "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init},
            "source_embed_weighted": {"class": "eval", "from": ["source_embed_raw"],
                                      "eval": "source(0) * %f" % self.embed_weight },
            "source_embed_with_pos": {"class": "positional_encoding", "add_to_input": True,
                                      "from": ["source_embed_weighted"]},
            "source_embed": {"class": "dropout", "from": ["source_embed_with_pos"], "dropout": self.embed_dropout},

            # encoder stack is added by separate function
            "encoder": {"class": "layer_norm", "from": ["enc_%02d" % self.encN]},

            "output": {"class": "rec", "from": [], "unit": {
                'output': {'class': 'choice', 'target': 'classes', 'beam_size': 12, 'from': ["output_prob"], "initial_output": 0}, # this is a vocab_id, make this flexible
                "end": {"class": "compare", "from": ["output"], "value": 0},
                'target_embed_raw': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['prev:output'],
                                     "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init},
            # there seems to be no <s> in t2t, they seem to use just the zero vector
                "target_embed_weighted": {"class": "eval", "from": ["target_embed_raw"],
                                          "eval": "source(0) * %f" % self.embed_weight },
                "target_embed_with_pos": {"class": "positional_encoding", "add_to_input": True,
                                          "from": ["target_embed_weighted"]},
                "target_embed": {"class": "dropout", "from": ["target_embed_with_pos"], "dropout": self.embed_dropout},


                # decoder stack is added by separate function
                "decoder": {"class": "layer_norm", "from": ["dec_%02d" % self.decN]},

                "output_prob": {
                    "class": "softmax", "from": ["decoder"], "dropout": 0.0,
                    "target": "classes", "loss": "ce", "loss_opts":
                        {"label_smoothing": self.label_smoothing, "use_normalized_loss": self.normalized_loss} if self.normalized_loss # ToDo: remove if in later version
                        else {"label_smoothing": self.label_smoothing},
                    "with_bias": False, "forward_weights_init": self.ff_init
                }

            }, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3"},

            "decision": {
                "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
                "loss_opts": {
                    # "debug_print": True
                }
            }

        }

        self.add_trafo_enc_layer(network, "source_embed", "enc_01")
        for n in range(1, self.encN):
            self.add_trafo_enc_layer(network, "enc_%02d" % n, "enc_%02d" % (n+1))

        self.add_trafo_dec_layer(network, network["output"]["unit"], "target_embed", "dec_01")
        for n in range(1, self.decN):
            self.add_trafo_dec_layer(network, network["output"]["unit"], "dec_%02d" % n, "dec_%02d" % (n+1),
                                     reuse_att='dec_01' if self.only_one_enc_dec_att else None)


        return network

network = TransformerNetwork().build()
search_output_layer = 'decision'
debug_print_layer_output_template = True
debug_add_check_numerics_on_output = True


# trainer
tf_log_memory_usage = True
log_batch_size = True
batching = 'random'
batch_size = 4700
max_seqs = 100
max_seq_length = 100  # 75

num_epochs = 500
model = 'net-model/network'
cleanup_old_models = True


learning_rate = 0.0003
optimizer = {'beta1': 0.9, 'beta2': 0.999, 'class': 'Adam', 'epsilon': 1e-08}
accum_grad_multiple_step = 4
gradient_clip = 0
gradient_noise = 0.0
learning_rate_file = 'newbob.data'
learning_rate_control = 'newbob_multi_epoch'
learning_rate_control_min_num_epochs_per_new_lr = 10
learning_rate_control_relative_error_relative_lr = True
newbob_relative_error_threshold = -0.005  # default is -0.01
newbob_multi_num_epochs = 20
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9



# log
log = "log/crnn.%s.log" % task
log_verbosity = 5
