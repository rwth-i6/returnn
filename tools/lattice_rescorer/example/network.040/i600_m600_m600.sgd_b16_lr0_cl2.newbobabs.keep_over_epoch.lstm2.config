#!returnn/rnn.py
# kate: syntax python;
# multisetup: finished True; finished_reason 'unstable';

import os
from subprocess import check_output
from returnn.util.basic import cleanup_env_var_path
cleanup_env_var_path("LD_LIBRARY_PATH", "/u/zeyer/tools/glibc217")

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn

data_files = {
    "train": "/work/asr3/irie/data/quaero_en/word/50M.RZ/quaero-transcriptions2011.txt.gz",
    "cv": "/work/asr3/irie/data/quaero_en/word/50M.RZ/quaero-eval12.50M.gz"}
vocab_file = "/work/asr3/irie/data/quaero_en/word/50M.RZ/vocab.txt"

#data_files = {
#    "train": "/work/asr2/irie/corpora/switchboard/data/train_unk.gz",
#    "cv": "/work/asr2/irie/corpora/switchboard/data/unk_replace/dev.unk.txt.gz",
#    "test": "/work/asr2/irie/corpora/switchboard/data/unk_replace/hub5_00.unk.txt.gz"}
#vocab_file = "/work/asr2/irie/corpora/switchboard/data/classes/vocab.30008.rwthlm"

orth_replace_map_file = ""
#orth_replace_map_file = "dependencies/orth_replace_map_file.js"

num_inputs = 127720  # via vocab_file

# train dataset len: LmDataset, sequences: 2392275, frames: NumbersDict(numbers_dict={'data': 26700802}, broadcast_value=0)
train_num_seqs = 57193
seq_order_bin_size = 100  # ??
train_epoch_split = 4  # ??

epoch_split = {"train": train_epoch_split, "cv": 1, "test": 1}
seq_order = {
    "train": "laplace:%i" % ((train_num_seqs // train_epoch_split) // seq_order_bin_size),
    "cv": "sorted",
    "test": "default"}

def get_dataset(data):
    assert data in ["train", "cv", "test"]
    return {
        "class": "LmDataset",
        "corpus_file": lambda: cf(data_files[data]),
        "orth_symbols_map_file": lambda: cf(vocab_file),
        "orth_replace_map_file": orth_replace_map_file,
        "word_based": True,
        "seq_end_symbol": "<sb>",
        "auto_replace_unknown_symbol": True,
        "unknown_symbol": "<unk>",
        "add_delayed_seq_data": True,
        "delayed_seq_data_start_symbol": "<sb>",
        "seq_ordering": seq_order[data],
        "partition_epoch": epoch_split[data]
    }

train = get_dataset("train")
dev = get_dataset("cv")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
#num_outputs = {"data": {"dim": num_inputs, "sparse": True, "dtype": "int32"}}  # sparse data
#num_outputs["delayed"] = num_outputs["data"]
extern_data = {
  "delayed": {"dim": num_inputs, "sparse": True, "dtype": "int32", "available_for_inference": True},
    "data": {"dim": num_inputs, "sparse": True, "dtype": "int32", "available_for_inference": False}
}
target = "data"
default_input = "delayed"
network = {
"input": {"class": "linear", "n_out": 600, "activation": "identity",
          "forward_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "from": ["data:delayed"]},
"lstm0": {"class": "rec", "unit": "nativelstm2", "initial_state" : "keep_over_epoch",
          "forward_weights_init" : "random_normal_initializer(mean=0.0, stddev=0.1)",
          "recurrent_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "n_out": 600, "dropout": 0., "L2": 0.0, "direction": 1, "from": ["input"]},
"lstm1": {"class": "rec", "unit": "nativelstm2", "initial_state" : "keep_over_epoch",
          "forward_weights_init" : "random_normal_initializer(mean=0.0, stddev=0.1)",
          "recurrent_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "n_out": 600, "dropout": 0., "L2": 0.0, "direction": 1, "from": ["lstm0"]},
"output": {"class": "softmax",
           "forward_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
           "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
           "loss": "ce", "target": "data", "from": ["lstm1"]}
}

# See also here for some hyperparameters:
# https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py

# config 1: 127. on cv
# max_seqs = 32
# gradient_clip_global_norm = 5.
# learning_rate = 0.005
# adam = True

# SGD config 2: 120. on cv
# max_seqs = 16
# gradient_clip_global_norm = 5.
# learning_rate = 1.
# epoch 25 / dev: 120.884027188 / test: 117.103220762

#network = {
#"input": {"class": "linear", "n_out": 128, "activation": "identity", "from": ["data:delayed"]},
#"lstm0": {"class": "rec", "unit": "lstm", "forward_weights_init" : "random_uniform_abs_initializer(0.01)",
# "recurrent_weights_init": "random_uniform_abs_initializer(0.01)",
# "n_out": 256, "dropout": 0., "L2": 0.0, "direction": 1, "from": ["input"]},
#"output": {"class": "softmax", "loss": "ce", "target": "data", "from": ["lstm0"]}
#}

batching = "random"
batch_size = 320
max_seq_length = batch_size
max_seqs = 16
chunking = "0"
#truncation = 15
num_epochs = 40
#pretrain = "default"
#pretrain_construction_algo = "from_input"
#gradient_clip = 1.
gradient_clip_global_norm = 2.
#gradient_clip = 0
#adam = True
gradient_noise = 0.
#learning_rate = 0.1
learning_rate = 1.
#learning_rate = 1.
learning_rate_control = "newbob_abs"
learning_rate_control_relative_error_relative_lr = True
newbob_multi_num_epochs = train_epoch_split
#newbob_multi_update_interval = 1

newbob_learning_rate_decay = 0.8
newbob_relative_error_threshold = 0
newbob_multi_update_interval = 1
learning_rate_control_error_measure = "dev_score_output:exp"


learning_rate_file = "newbob.data"
model = "net-model/network"

calculate_exp_loss = True
cleanup_old_models = True

# log
log = "log/returnn.%s.log" % task
log_verbosity = 5

