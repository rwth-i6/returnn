#!returnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
from returnn.tensor import Tensor, Dim, batch_dim, TensorDict
import returnn.frontend as rf
from returnn.util.basic import get_login_username

demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

backend = "torch"

task = "train"
train = {"class": "Task12AXDataset", "num_seqs": 1000}
dev = {"class": "Task12AXDataset", "num_seqs": 100, "fixed_random_seed": 1}

time_dim = Dim(None, name="time")
in_dim = Dim(9, name="in")
out_dim = Dim(2, name="out")
extern_data = {
    "data": {"dims": (batch_dim, time_dim, in_dim), "dtype": "float32"},
    "classes": {"dims": (batch_dim, time_dim), "sparse_dim": out_dim, "dtype": "int32", "available_for_inference": False},
}
model_outputs = {
    "output": {"dims": (batch_dim, time_dim, out_dim), "dtype": "float32"},
}

batching = "random"
batch_size = 5000
max_seqs = 10


class Model(rf.Module):
    def __init__(self):
        super().__init__()
        hidden_dim = Dim(50, name="hidden")
        hidden2_dim = Dim(100, name="hidden2")
        self.layers = rf.ModuleList(
            rf.Conv1d(in_dim, hidden_dim, 5, padding="same"),
            rf.Conv1d(hidden_dim, hidden2_dim, 5, padding="same"),
            rf.Conv1d(hidden2_dim, out_dim, 5, padding="same"),
        )

    def __call__(self, x: Tensor, *, spatial_dim: Dim):
        for layer in self.layers[:-1]:
            x, _ = layer(x, in_spatial_dim=spatial_dim)
            x = rf.relu(x)
        x, _ = self.layers[-1](x, in_spatial_dim=spatial_dim)
        return x  # logits


def get_model(*, epoch, step, **_kwargs):
    return Model()


def forward_step(*, model: Model, extern_data: TensorDict):
    """
    Function used in inference.
    """
    data = extern_data["data"]
    out = model(data, spatial_dim=time_dim)
    out.mark_as_default_output(shape=(batch_dim, time_dim, out_dim))


def train_step(*, model: Model, extern_data: TensorDict, **_kwargs):
    """
    Function used in training/dev.
    """
    data = extern_data["data"]
    logits = model(data, spatial_dim=time_dim)
    logits_packed, pack_dim = rf.pack_padded(logits, dims=(batch_dim, time_dim), enforce_sorted=False)
    targets = extern_data["classes"]
    targets_packed, _ = rf.pack_padded(targets, dims=(batch_dim, time_dim), enforce_sorted=False, out_dim=pack_dim)
    loss = rf.cross_entropy(estimated=logits_packed, estimated_type="logits", target=targets_packed, axis=out_dim)
    loss.mark_as_loss(name="ce")
    best = rf.reduce_argmax(logits_packed, axis=out_dim)
    frame_error = best != targets_packed
    frame_error.mark_as_loss(name="fer", as_error=True)


# training
optimizer = {"class": "adam"}

learning_rate = 0.01
learning_rate_control = "newbob"
learning_rate_decay = 0.9
newbob_relative_error_threshold = 0.0
learning_rate_file = "/tmp/%s/returnn/%s/learning_rates" % (get_login_username(), demo_name)

model = "/tmp/%s/returnn/%s/model" % (get_login_username(), demo_name)
num_epochs = 5

# log
#log_verbosity = 3
log_verbosity = 5
