#!returnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

# https://www.cs.toronto.edu/~graves/preprint.pdf
# Graves BLSTM-CTC without LM on 61 phones: 30.51% PER or 31.47% PER (best path decoding)
# BLSTM has 1 layer.
# Gaussian input noise.

# https://arxiv.org/pdf/0804.3269.pdf
# Graves BLSTM-CTC with LM on 39 phones, 24.58% PER
# input dim 39, 128 cells each LSTM dir, 1 layer

import os
from subprocess import check_output
import numpy
from returnn.util.basic import get_login_username
from returnn.datasets.generating import TimitDataset

demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

debug_mode = False
if int(os.environ.get("DEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# task
use_tensorflow = True
task = "train"
#device = "gpu"  # auto if commented out
multiprocessing = True
update_on_device = True

# data, see TimitDataset and NltkTimitDataset
# returnn/tools/dump-dataset.py "{'class': 'TimitDataset', 'timit_dir': '...'}"
# returnn/tools/dump-dataset.py "{'class': 'NltkTimitDataset'}"
# Set to None if you don't have. It will use the data from NLTK then.
# Note that the NLTK TIMIT data is only a very small subset of TIMIT.
raw_timit_dir = "/u/corpora/speech/timit/raw/cdtimit"
raw_timit_dir = None
num_outputs = {'data': (40*2, 2), 'classes': (61, 1)}
num_inputs = num_outputs["data"][0]
if raw_timit_dir:
    num_seqs = {'train': 3696, 'dev': 192}
else:
    num_seqs = {'train': 144, 'dev': 16}

EpochSplit = 1
SeqOrderTrainBins = num_seqs["train"] // 10
TrainSeqOrder = "laplace:%i" % SeqOrderTrainBins
if debug_mode:
    TrainSeqOrder = "default"

def get_dataset(data):
    epochSplit = {"train": EpochSplit}.get(data, 1)
    d = {
        "class": "TimitDataset" if raw_timit_dir else "NltkTimitDataset",
        "with_delta": True,
        "train": (data == "train"),
        "seq_ordering": {"train": TrainSeqOrder, "dev": "sorted"}.get(data, "default"),
        "estimated_num_seqs": (num_seqs.get(data, None) // epochSplit) if data in num_seqs else None}
    if raw_timit_dir:
        d["timit_dir"] = raw_timit_dir
    return d

train = get_dataset("train")
dev = get_dataset("dev")
#dev_short = get_dataset("dev_short")
#test_data = get_dataset("test")
#eval_data = get_dataset("eval")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
network = {
"fwd0": {"class": "rec", "unit": "nativelstm2", "direction": 1, "L2": 0.01, "dropout": 0., "n_out": 250},
"bwd0": {"class": "rec", "unit": "nativelstm2", "direction": -1, "L2": 0.01, "dropout": 0., "n_out": 250},
"fwd1": {"class": "rec", "unit": "nativelstm2", "direction": 1, "L2": 0.01, "dropout": 0., "n_out": 250, "from": ["fwd0", "bwd0"]},
"bwd1": {"class": "rec", "unit": "nativelstm2", "direction": -1, "L2": 0.01, "dropout": 0., "n_out": 250, "from": ["fwd0", "bwd0"]},
"fwd2": {"class": "rec", "unit": "nativelstm2", "direction": 1, "L2": 0.01, "dropout": 0., "n_out": 250, "from": ["fwd1", "bwd1"]},
"bwd2": {"class": "rec", "unit": "nativelstm2", "direction": -1, "L2": 0.01, "dropout": 0., "n_out": 250, "from": ["fwd1", "bwd1"]},
"output": {"class": "softmax", "loss": "ctc", "target": "classes", "from": ["fwd2", "bwd2"], "darc1": 0.001},

"ler_39": {"class": "copy", "from": ["output"], "only_on_eval": True,
    "loss": "edit_distance", "loss_opts": {"ctc_decode": True, "label_map": TimitDataset.get_label_map()}}
}

# trainer
batching = "random"
batch_size = 2000
max_seqs = 64
max_seq_length = batch_size
#chunking = ""  # no chunking
truncation = -1
num_epochs = 100
#model = "net-model/network"
model_dir = "/tmp/%s/returnn/%s" % (get_login_username(), demo_name)
model = "%s/model" % model_dir
cleanup_old_models = True
learning_rate_file = "%s/newbob.data" % model_dir
#save_interval = 2
#pretrain = "default"
#pretrain_construction_algo = "from_input"
#gradient_clip = 0
#gradient_nan_inf_filter = True
optimizer = {"class": "nadam"}
#optimizer_epsilon = 1e-8
#debug_add_check_numerics_ops = True
#debug_add_check_numerics_on_output = True
tf_log_memory_usage = True
gradient_noise = 0.1
#learning_rate = 0.0005
learning_rate = 0.001
#learning_rate = 0.005
learning_rate_control = "newbob_multi_epoch"
#learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
newbob_learning_rate_decay = 0.8
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1

# log
#log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/returnn.seq-train.%s.log" % task
#log = "log/returnn.%s.log" % task
log_verbosity = 5
