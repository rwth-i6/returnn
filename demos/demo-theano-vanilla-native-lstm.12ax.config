#!returnn.py
# kate: syntax python;

import os
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

task = "train"
train = {"class": "Task12AXDataset", "num_seqs": 1000}
dev = {"class": "Task12AXDataset", "num_seqs": 100, "fixed_random_seed": 1}

num_inputs = 9
num_outputs = 2
batching = "random"
batch_size = 5000
max_seqs = 10
chunking = "200:200"

network = {
"fw0": {"class": "native_lstm", "n_out": 10},
"output": {"class": "softmax", "loss": "ce", "from": ["fw0"]}
}

# training
adam = True
learning_rate = 0.01
model = "/tmp/returnn.%s.network" % demo_name
num_epochs = 100
save_interval = 20
gradient_clip = 0

# log
log_verbosity = 3

# example out:
"""
net params #: 822
net trainable params: [W_in_data_fw0, W_re_fw0, b_fw0, W_in_fw0_output, b_output]
start training at epoch 1 and batch 0
epoch 1 score: 0.249362083515 elapsed: 0:00:03 dev: score 0.0969276792082 error 0.0316673025563
epoch 2 score: 0.0287555100658 elapsed: 0:00:03 dev: score 0.0167580631934 error 0.00209843571156
epoch 3 score: 0.00852688688292 elapsed: 0:00:03 dev: score 0.00641669734932 error 0.000763067531477
epoch 4 score: 0.00674126969606 elapsed: 0:00:03 dev: score 0.00401622417409 error 0.000953834414346
"""
