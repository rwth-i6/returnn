#!returnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

task = "train"
train = {"class": "Task12AXDataset", "num_seqs": 1000}
dev = {"class": "Task12AXDataset", "num_seqs": 100, "fixed_random_seed": 1}

num_inputs = 9
num_outputs = 2
batching = "random"
batch_size = 5000
max_seqs = 10
chunking = "200:200"

network = {
"fw0": {"class": "rec", "unit": "lstmp", "n_out": 10},  # This one uses our optimized LSTM CUDA kernel.
"output": {"class": "softmax", "loss": "ce", "from": ["fw0"]}
}

# training
adam = True
learning_rate = 0.01
model = "/tmp/returnn.%s.network" % demo_name
num_epochs = 100
save_interval = 20
gradient_clip = 0

# log
log_verbosity = 3

# example out:
"""
net params #: 822
net trainable params: [W_in_data_fw0, W_re_fw0, b_fw0, W_in_fw0_output, b_output]
start training at epoch 1 and batch 0
epoch 1 score: 0.280451681915 elapsed: 0:00:03 dev: score 0.145912614318 error 0.0425410148798
epoch 2 score: 0.053846877154 elapsed: 0:00:03 dev: score 0.020243201898 error 0.00400610454025
epoch 3 score: 0.0117291950473 elapsed: 0:00:03 dev: score 0.00935518614257 error 0.00209843571156
"""
