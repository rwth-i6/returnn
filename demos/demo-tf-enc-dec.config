#!returnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

"""
This example shows a simple encoder decoder (without attention),
and demonstrates the RecLayer, and the automatic optimization to move layers outside the loop.

Run::

    python3 rnn.py demos/demo-tf-enc-dec.config

Run without loop optimization::

    python3 rnn.py demos/demo-tf-enc-dec.config ++optimize_move_layers_out 0

"""

import os
from returnn.util.basic import get_login_username
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

# task
use_tensorflow = True
task = "train"

# data
#train = {"class": "TaskEpisodicCopyDataset", "num_seqs": 1000}
#num_inputs = 10
#num_outputs = 10
#train = {"class": "TaskXmlModelingDataset", "num_seqs": 1000}
#num_inputs = 12
#num_outputs = 12
# TaskNumberBaseConvertDataset is maybe a better example dataset, as the source and target have diff lengths.
train = {"class": "TaskNumberBaseConvertDataset", "num_seqs": 10000}
out_dim = 2
extern_data = {
    "data": {"sparse": True, "dim": 8},
    "classes": {"sparse": True, "dim": out_dim}
}

dev = train.copy()
dev.update({"num_seqs": train["num_seqs"] // 10, "fixed_random_seed": 42})
layer_dim = config.int("layer_dim", 500)

# network
# (also defined by num_inputs & num_outputs)
network = {
    "input_embed": {"class": "linear", "activation": None, "from": "data", "n_out": layer_dim},  # if sparse input
    "input": {"class": "rec", "unit": "nativelstm2", "from": "input_embed", "n_out": layer_dim, "L2": 0.01},
    "input_last": {"class": "get_last_hidden_state", "from": "input", "n_out": layer_dim * 2},

    "output": {"class": "rec", "from": [], "unit": {
        "output_embed": {"class": "linear", "activation": None, "from": "output", "n_out": layer_dim},
        "s": {"class": "rec", "unit": "nativelstm2", "from": ["prev:output_embed", "base:input_last"], "n_out": layer_dim},
        "prob": {"class": "softmax", "from": "s", "target": "classes0", "loss": "ce"},
        "output": {"class": "choice", "from": "prob", "target": "classes0", "beam_size": 8},
        "end": {"class": "compare", "from": "output", "value": out_dim}
    }, "target": "classes0"},

    "target_with_eos_": {"class": "reinterpret_data", "from": "data:classes", "increase_sparse_dim": 1},
    "target_with_eos": {
        "class": "postfix_in_time", "postfix": out_dim, "from": "target_with_eos_",
        "register_as_extern_data": "classes0"}
}

debug_print_layer_output_template = True

# trainer
batching = "random"
batch_size = 5000
max_seqs = 40
chunking = "0"
optimizer = {"class": "adam"}
learning_rate = 0.0005
learning_rate_control = "newbob"
learning_rate_control_relative_error_relative_lr = True
model = "/tmp/%s/returnn/%s/model" % (get_login_username(), demo_name)  # https://github.com/tensorflow/tensorflow/issues/6537
num_epochs = 100
save_interval = 20

# log
log_verbosity = 4
tf_log_memory_usage = True
log_batch_size = True
