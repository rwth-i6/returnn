#!returnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import torch
from torch import nn
import os
from returnn.util.basic import get_login_username

demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

backend = "torch"

task = "train"
train = {"class": "Task12AXDataset", "num_seqs": 1000}
dev = {"class": "Task12AXDataset", "num_seqs": 100, "fixed_random_seed": 1}

num_inputs = 9
num_outputs = 2
batching = "random"
batch_size = 5000
max_seqs = 10
chunking = "200:200"

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(num_inputs, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        return self.linear_relu_stack(x)

def get_model(**_kwargs):
    return Model()

def train_step(*, model: Model, data, train_ctx, **_kwargs):
    logits = model(data["data"])
    logits_packed = torch.nn.utils.rnn.pack_padded_sequence(logits, data["data:seq_len"], batch_first=True, enforce_sorted=False)
    targets = data["classes"]
    targets_packed = torch.nn.utils.rnn.pack_padded_sequence(targets, data["data:seq_len"], batch_first=True, enforce_sorted=False)
    loss = nn.CrossEntropyLoss()(logits_packed.data, targets_packed.data.long())
    train_ctx.mark_as_loss(loss)


# training
# TODO maybe remove this...
optimizer = {"class": "adam"}
learning_rate = 0.01

model = "/tmp/%s/returnn/%s/model" % (get_login_username(), demo_name)
num_epochs = 5

# log
#log_verbosity = 3
log_verbosity = 5
